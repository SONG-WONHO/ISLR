{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ISLR] Chapter 03 - Linear Regression\n",
    "\n",
    "* ISLR 책을 공부하며 가벼운 내용 소개, 원래 알고 있던 내용을 함께 정리합니다.\n",
    "* ISLR 책에 대한 원서는 [여기](http://www-bcf.usc.edu/~gareth/ISL/)에서 확인할 수 있습니다.\n",
    "* ISLR 책에 대한 참고 블로그는 [Go`s Blog](https://godongyoung.github.io/category/ML.html)를 참고했습니다.\n",
    "* 코드 구현은 [밑바닥부터 시작하는 데이터 과학](https://book.naver.com/bookdb/book_detail.nhn?bid=10652749) 이 책을 참고했습니다.\n",
    "* 오타나 틀린 내용, 토의할 내용은 언제나 이슈로 등록해주세요.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "    3.1 Simple Linear Regression\n",
    "        3.1.1 Estimating the Coefficients\n",
    "        3.1.2 Assessing the Accuracy of the Coefficient Estimates\n",
    "        3.1.3 Assessing the Accuracy of the Model\n",
    "    \n",
    "    3.2 Multiple Linear Regression\n",
    "        3.2.1 Estimating the Regression Coefficients\n",
    "        3.2.2 Some Important Questions\n",
    "    \n",
    "    3.3 Other Considerations in the Regression Model\n",
    "        3.3.1 Qualitative Predictors\n",
    "        3.3.2 Extensions of the Linear Model\n",
    "        3.3.3 Potential Problems\n",
    "        \n",
    "    3.4 The Marketing Plan\n",
    "    \n",
    "    3.4 Comparison of Linear Regression with K-Nearest Neighbors\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형회귀는 지도학습 중 비교적 간단한 방법에 속하는 모델이다. 다른 모델에 비해 비교적 간단하지만 해석력 및 다양한 분석을 할 수 있어 널리 사용되며 다른 방법의 기초가 되는 모델이다. 특히 딥러닝은 선형회귀부터 시작이니 알아두면 알아둘수록 좋다.\n",
    "\n",
    "다만, 나는 통계 전공자가 아니다보니 상대적으로 부족한 부분이 많다. 이 장을 읽을때 명확하지 않은 부분이 많았다. 이를 보충하기 위해 최근 통계강의를 듣고 있으니 부족한 부분은 차차 채울 수 있도록 하겠다.\n",
    "\n",
    "독립변수와 종속변수의 관계, 예를 들어 TV, 라디오, 뉴스 광고 지출과 총 판매량의 관계를 밝히고자 할 때 다음 주요 질문을 할 수 있다.\n",
    "\n",
    "* 1. 실제로 광고 지출과 총 판매량 사이에 어떠한 관계가 있는가?\n",
    "* 2. 광고 지출과 총 판매량 사이에 관계가 있다면 얼마나 뚜렷한가?\n",
    "* 3. 어떤 매체(TV, 라디오, 뉴스)가 총 판매량에 기여를 했는가?\n",
    "* 4. 각 매체가 총 판매량에 미친 영향을 얼마나 정확하게 추정할 수 있는가?\n",
    "* 5. 미래의 총 판매량을 얼마나 정확하게 예측할 수 있는가?\n",
    "* 6. 광고 지출과 총 판매량 사이에 선형 관계가 있는가?\n",
    "* 7. 각 매체 사이에 상호작용 효과가 있는가?\n",
    "    \n",
    "이와 같은 질문에 선형회귀가 어떠한 답을 내줄 수 있는 지 알아볼 것이다.\n",
    "\n",
    "앞서 2장에서 우리는 추론과 예측을 위해 어떤 함수를 추정한다고 배웠다. 이 관점에서 선형회귀는 상당히 추론 관점의 통계적 학습 모델이다. 위 질문 중 5번을 제외한 나머지가 추론 관점의 질문이다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simple Linear Regression\n",
    "---\n",
    "독립변수와 종속변수의 관계를 설명할 때, 가장 단순한 모델 중 하나이다. 단순 선형회귀 라는 용어에서 알 수 있듯이, 실제 식을 보면 매우 간단하다. 다만, 선형회귀를 위해서 엄청난 가정과 제약이 들어간다. 컴피티션에선 딱히 가정을 만족하지 않아도 사용하는 경향이 있지만, 사실 많은 가정들을 만족시키지 못하면 실제로는 사용할 수 없다고 한다. 아래 식으로 보자.\n",
    "\n",
    "\\begin{equation*}\n",
    "Y = \\beta_0 + \\beta_1X + e\n",
    "\\end{equation*}\n",
    "\n",
    "위 식에서 $X$가 독립변수, $Y$가 $X$의 변화에 따른 종속변수이다. ML관점에서 $X$는 피처, $Y$는 타겟이다. 이를 영어로는 **'regression Y on X'** 라고 표현한다. 이때, $\\beta_0$와 $\\beta_1$은 계수 또는 **파라미터**라고 지칭한다. 우리가 함수를 추정한다는 것은 결국 이 파라미터를 추정한다는 것과 같다. \n",
    "\n",
    "번외로 연구자들마다 파라미터에 대한 다양한 표기법을 사용한다. 내가 봤을 때 주로 $\\theta$ 또는 $\\beta$를 사용하는 것 같다.\n",
    "\n",
    "선형 함수를 그래프로 보면 아래와 같다. 고등학교 때 일차함수에 대해 배웠다면 매우 익숙한 그래프일 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e6706574e0>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(5)\n",
    "y = range(5)\n",
    "\n",
    "f, ax = plt.subplots(figsize = (4,4))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### key sentences\n",
    "\n",
    ">* Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X.\n",
    "* Together, $\\beta_0$ and $\\beta_1$ are known as the model coefficients or parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Estimating the Coefficients\n",
    "우리는 데이터를 잘 설명해 줄 선형함수 한 가지를 가정했다. 이제 해야할 일은 주어진 데이터에 맞춰 선형함수의 파라미터를 추정해야한다. 앞서 2장에서 우리는 함수를 추정하는 방법에 대해 배웠다. 다시한번 떠올려보자.\n",
    "\n",
    "\\begin{equation*}\n",
    "argmin_{f \\subset H} \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x^i), y^i)\n",
    "\\end{equation*}\n",
    "\n",
    "함수공간 H의 함수 f를 정의해 예측값과 실제값 사이의 손실함수를 정의하고, 손실함수를 최소화 할 수 있는 함수 f를 우리는 채택해야 한다. 일단, 선형회귀에서 함수는 선형함수로 함수공간이 축소되고, 이제 정의해야 할 것은 손실함수이다. \n",
    "\n",
    "선형회귀의 경우 손실함수로 주로 ***RSS(Sum of squares residual)*** 또는 ***SSE(Sum of square error)***를 사용한다(RSS와 SSE는 표기만 다르지 같은 의미다.). \n",
    "\n",
    "\\begin{equation*}\n",
    "RSS = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 ... + (y_n - \\hat{y}_n)^2\n",
    "\\end{equation*}\n",
    "\n",
    "일반적으로 $y_i - \\hat{y}_i$을 잔차라고 하며, $e_i$으로 표기한다.\n",
    "\n",
    "그렇다면 RSS를 최소화하는 파라미터를 어떻게 구할 수 있을까? 위 식을 풀어보면 파라미터의 제곱식이 된다. 간단히 미분해서 0이 되는 파라미터를 찾으면 된다. 추정된 계수는 다음과 같다.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_1} = \\frac{\\sum x_i(y_i - \\bar{y})}{\\sum x_i(x_i - \\bar{x})} \n",
    "\\end{equation*}\n",
    "\n",
    "위 방법은 미분을 통해 주어진 데이터에 딱 맞는 회귀계수를 바로 추정한 것이고 향후 경사하강법 등 다양한 방법들을 통해 회귀계수를 추정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "회귀계수 b0: 1.38 \n",
      "회귀계수 b1: 0.85\n",
      "선형함수: y_hat = 0.85 * x + 1.38\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array(range(10))\n",
    "y = np.array([1,3,5,2,4,6,7,5,9,10])\n",
    "\n",
    "b1 = np.sum(x * (y - np.mean(y))) / np.sum(x * (x - np.mean(x)))\n",
    "b0 = np.mean(y) - b1 * np.mean(x)\n",
    "\n",
    "f, ax = plt.subplots(figsize = (8,4))\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, b1 * x + b0)\n",
    "\n",
    "print(\"회귀계수 b0: {0:.2f} \\n회귀계수 b1: {1:.2f}\".format(b0, b1))\n",
    "print(\"선형함수: y_hat = {0:.2f} * x + {1:.2f}\".format(b1, b0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key sentences\n",
    "> * in other words, we want to find a $\\beta_0$ and a $\\beta_1$ such that the resulting line is as close as possible data points.\n",
    "* By far the most common approach involves minimizing the least squares criterion, and we take that approach in this chapter.\n",
    "* The least squares approach chooses $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ to minimize the ***RSS***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Assessing the Accuracy of the Coefficient Estimates\n",
    "---\n",
    "\n",
    "통계에 대한 내용이 많아요 ... 곧 통계 강의를 듣고 채우겠습니다 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Assessing the Accuracy of the Model\n",
    "---\n",
    "앞서 최소제곱법을 통해 추정한 함수에 대한 평가이다. 2장에서 말했듯이 함수에 대한 평가 지표는 매우 다양하다.\n",
    "\n",
    "그 중 선형함수를 평가할 때 자주 사용하는 ***RSE(residual standard error)***와 $R^2$에 대해 알아볼 것이다.\n",
    "\n",
    "#### RSE\n",
    "RSE는 앞서 배운 **MSE**의 제곱근을 의미한다. 일반적으로 **RMSE(root mean squared error)**로 자주 사용한다.\n",
    "\n",
    "\\begin{equation*}\n",
    "RSE(RMSE) = \\sqrt{\\frac{1}{n-2}RSS}\n",
    "\\end{equation*}\n",
    "\n",
    "위 식을 보면 ***RSE***를 데이터의 개수 $n$이 아닌 $n-2$로 나눈다. 이는 사실, RSS가 $\\sigma^2$의 추정으로 사용될 수 있고, $\\sigma^2$의 추정에 있어 추정계수만큼 자유도를 감소해야 한다($\\sigma^2$는 오류 항에 대한 분산). 하지만, ML함에 있어 만 개 이상의 데이터를 다루는 경우가 많고 10,000으로 나누나 9,998로 나누나 거의 비슷하기 때문에 데이터 개수만큼 나눠서 많이 사용한다. 하지만 RSE 같은 경우 종속변수의 단위에 따라 달라질 수 있어 다양한 데이터 셋을 객관적으로 평가하기 힘들다는 단점이 있다고는 한다. 하지만, 우리는 보통 한 종류의 데이터 셋을 다른 모델로 평가할 때 평가 지표를 사용하기 때문에 단점이 사실상 없다.\n",
    "\n",
    "#### $R^2$\n",
    "$R^2$는 비율로 모델을 평가하기 때문에 종속변수에 단위에 상관없이 일관성있는 평가지표이다. 그렇다면 어떤 비율로 모델을 평가할까? 전체 에러 중에 모델이 설명하는 에러 비율을 의미한다. 이때 전체 에러를 실제값과 실제값 평균의 차 즉, 종속변수에 대한 분산으로 정의하며, 이 중 모델이 설명하지 못하는 에러는 예측값과 실제값의 차로 정의한다. 아래 그래프를 보자.\n",
    "\n",
    "![그림](https://www.researchgate.net/profile/Christian_Gold2/publication/322398615/figure/fig17/AS:581620512903169@1515680544091/Visualization-of-SSE-SSR-SST.png)\n",
    "\n",
    "(Reference: Modeling of Take-Over Performance in Highly Automated Vehicle Guidance - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/Visualization-of-SSE-SSR-SST_fig17_322398615)\n",
    "\n",
    "위 그래프에서 ***SST(Sum of Total) 또는 TSS***는 잔차를 의미하고, ***SSE***는 ***RSS*** 즉, 우리가 앞서 배운 예측값과 실제값의 차를 의미한다(***SST = TSS***, ***RSS = SSE***). ***SSR***은 ***SST***에서 ***SSE***를 뺀 값이다. $R^2$에서는 $SST$ 대비 우리의 모델이 설명하는 에러 즉, $SSR$의 비율이다.\n",
    "\n",
    "\\begin{equation*}\n",
    "R^2 = \\frac{SSR}{TSS} = \\frac{TSS - SSE}{TSS} = 1 - \\frac{SSE}{TSS}\n",
    "\\end{equation*}\n",
    "\n",
    "따라서 $R^2$값은 0과 1사이의 범위를 가지며, 1에 가까울수록 우리의 모델이 종속변수를 잘 설명하고, 0에 가까울수록 우리의 모델이 종속변수를 잘 설명하지 못한다는 의미이다.\n",
    "\n",
    "그렇다면 $R^2$가 분야에 상관없이 절대적인 지표로 활용될 수 있을까? 아니다! 어떤 분야에선 $R^2$가 0.9정도만 돼도 문제가 있을 수 있지만, 또 다른 분야에선 $R^2$가 0.1 미만이어도 좋은 분석일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple Linear Regression\n",
    "---\n",
    "단순 선형회귀는 설명력에 있어서 매우 강력하다. 종속변수에 영향을 미치는 독립변수가 한 개 뿐이니 독립변수가 종속변수에 어떤 영향을 미치는 지 정확하게 확인할 수 있다. 하지만, 현실세계에서 독립변수가 한 개인 상황은 존재하지 않는다. 어떤 현상은 복합적인 요인에 의해 결정된다. 예를 들어, 사람을 구분하는 문제를 푼다고 했을 때, 사람이 키 만으로 결정되지는 않는다. 키, 몸무게, 안경 착용 여부, 머리 색 등등 정말 복합적인 요인이 한 사람을 결정한다. 이처럼 독립변수가 여러개 있을 때, 우리는 다중선형회귀라 한다. \n",
    "\n",
    "참고로 다중선형회귀와 다항선형회귀는 다르다. 마치 독립변수가 여러 개 있는 것이 다항선형회귀로 착각하기 쉬운데 다중선형회귀다. 영어로 이해하면 훨씬 쉽다. 다중선형회귀는 **Multiple Linear Regression**, 다항선형회귀는 **Polynomial Linear Regression**이다!\n",
    "\n",
    "그렇다면 다중선형회귀의 경우 어떻게 접근해야 할까? 우리는 단순 선형회귀를 배웠으니 독립변수의 개수만큼 단순 선형회귀 모델을 만들어 볼 수 있다. 하지만, 이 경우 크게 두 가지 문제가 발생한다. 먼저 독립변수가 $n$개라면 우리의 모델이 $n$개가 된다. 그렇다면 예측문제에서 모델 결정 문제가 발생한다. 둘째, 종속변수는 한 가지 독립변수에 의해서 결정되는 것이 아닌, 여러가지 독립변수에 의해 결정된다. 이때 각 독립변수 별 모델을 만든다면, 나머지 독립변수를 무시하게 된다.\n",
    "\n",
    "그렇다면 어떻게 선형모델을 작성해야 할까? 아래 식을 보자.\n",
    "\n",
    "\\begin{equation*}\n",
    "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ... \\beta_pX_p + e\n",
    "\\end{equation*}\n",
    "\n",
    "위 식에서 $\\beta_i$가 의미하는 것은 종속변수 $Y$에 $X_i$가 미치는 영향이다. 중요한 점은 모든 다른 독립변수가 고정됐을 때 미치는 영향이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key sentences\n",
    "\n",
    ">* However, in practice we often have more than one predictor.\n",
    "* A better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors.\n",
    "* We interpret $beta_i$ as the average effect on Y of a one unit increase in $X_i$, holding all other predictors fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Estimating the Regression Coefficients\n",
    "---\n",
    "다중선형회귀에서 계수 추정은 어렵지 않다. 단순 선형회귀에서 추정했던 것처럼 손실함수를 정의하고, 손실함수를 최소화하는 파라미터를 찾으면 된다. 다중선형회귀 최소제곱법을 통해 손실함수를 정의한다.\n",
    "\n",
    "\\begin{equation*}\n",
    "RSS = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 ... + (y_n - \\hat{y}_n)^2\n",
    "\\end{equation*}\n",
    "\n",
    "***RSS***를 최소화하는 파라미터 역시 편미분을 통해 구할 수 있다. 다만, 모든 계수마다 편미분을 계산하려면 엄청난 시간과 노력이 필요하다. 행렬식을 이용하면 정규 방정식을 통해 쉽고 편하게 계산할 수 있다. 다만, 회귀계수가 많아지면 많아질수록 정규방정식의 계산속도는 기하급수적으로 늘어난다. 고로 딥러닝에서 활용하기에는 부적합하다.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\overrightarrow{Y} = X\\overrightarrow{\\beta}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\overrightarrow{\\beta} = (X^TX)^{-1}X^T\\overrightarrow{Y}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Some Important Questions\n",
    "---\n",
    "우리가 다중선형회귀를 공부할 때 만나게되는 주요 질문들이 있다.\n",
    "* 종속변수를 예측할 때 독립변수 중 최소 한 개라도 유용한가? (회귀계수의 적합성 문제)\n",
    "* 종속변수를 예측할 때 모든 독립변수가 도움이 되는가 또는 독립변수 중 일부만 도움이 되는가?\n",
    "* 모델이 얼마나 데이터에 잘 적합 됐는가? (모델 평가 문제)\n",
    "* 우리의 예측은 얼마나 정확할 것인가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
